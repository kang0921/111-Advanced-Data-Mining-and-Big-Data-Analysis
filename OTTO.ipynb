{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kang0921/1111-Advanced-Data-Mining-and-Big-Data-Analysis/blob/main/OTTO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper Parameter Tuning\n",
        "# 0:clicks 1:carts 2:orders\n",
        "type_weight = {0:0.5,\n",
        "               1:9,\n",
        "               2:0.5}\n",
        "type_weight_multipliers = type_weight\n",
        "\n",
        "clicks_th = 15\n",
        "carts_th  = 20 \n",
        "orders_th = 20\n",
        "\n",
        "VER = 7"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-31T07:14:29.76643Z",
          "iopub.execute_input": "2022-12-31T07:14:29.76688Z",
          "iopub.status.idle": "2022-12-31T07:14:29.773163Z",
          "shell.execute_reply.started": "2022-12-31T07:14:29.766844Z",
          "shell.execute_reply": "2022-12-31T07:14:29.771993Z"
        },
        "trusted": true,
        "id": "SbTpRK8n3MSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The following is an appropriation of CHRIS DEOTTE's notebook. Thank you!\n",
        "### https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575\n",
        "---"
      ],
      "metadata": {
        "id": "RxlxyF4s3MSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Candidate ReRank Model using Handcrafted Rules\n",
        "In this notebook, we present a \"candidate rerank\" model using handcrafted rules. We can improve this model by engineering features, merging them unto items and users, and training a reranker model (such as XGB) to choose our final 20. Furthermore to tune and improve this notebook, we should build a local CV scheme to experiment new logic and/or models.\n",
        "\n",
        " UPDATE: I published a notebook to compute validation score [here][10] using Radek's scheme described [here][11].\n",
        "\n",
        "Note in this competition, a \"session\" actually means a unique \"user\". So our task is to predict what each of the `1,671,803` test \"users\" (i.e. \"sessions\") will do in the future. For each test \"user\" (i.e. \"session\") we must predict what they will `click`, `cart`, and `order` during the remainder of the week long test period.\n",
        "\n",
        "### Step 1 - Generate Candidates\n",
        "For each test user, we generate possible choices, i.e. candidates. In this notebook, we generate candidates from 5 sources:\n",
        "* User history of clicks, carts, orders\n",
        "* Most popular 20 clicks, carts, orders during test week\n",
        "* Co-visitation matrix of click/cart/order to cart/order with type weighting\n",
        "* Co-visitation matrix of cart/order to cart/order called buy2buy\n",
        "* Co-visitation matrix of click/cart/order to clicks with time weighting\n",
        "\n",
        "### Step 2 - ReRank and Choose 20\n",
        "Given the list of candidates, we must select 20 to be our predictions. In this notebook, we do this with a set of handcrafted rules. We can improve our predictions by training an XGBoost model to select for us. Our handcrafted rules give priority to:\n",
        "* Most recent previously visited items\n",
        "* Items previously visited multiple times\n",
        "* Items previously in cart or order\n",
        "* Co-visitation matrix of cart/order to cart/order\n",
        "* Current popular items\n",
        "\n",
        "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Nov-2022/c_r_model.png)\n",
        "  \n",
        "# Credits\n",
        "We thank many Kagglers who have shared ideas. We use co-visitation matrix idea from Vladimir [here][1]. We use groupby sort logic from Sinan in comment section [here][4]. We use duplicate prediction removal logic from Radek [here][5]. We use multiple visit logic from Pietro [here][2]. We use type weighting logic from Ingvaras [here][3]. We use leaky test data from my previous notebook [here][4]. And some ideas may have originated from Tawara [here][6] and KJ [here][7]. We use Colum2131's parquets [here][8]. Above image is from Ravi's discussion about candidate rerank models [here][9]\n",
        "\n",
        "[1]: https://www.kaggle.com/code/vslaykovsky/co-visitation-matrix\n",
        "[2]: https://www.kaggle.com/code/pietromaldini1/multiple-clicks-vs-latest-items\n",
        "[3]: https://www.kaggle.com/code/ingvarasgalinskas/item-type-vs-multiple-clicks-vs-latest-items\n",
        "[4]: https://www.kaggle.com/code/cdeotte/test-data-leak-lb-boost\n",
        "[5]: https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic\n",
        "[6]: https://www.kaggle.com/code/ttahara/otto-mors-aid-frequency-baseline\n",
        "[7]: https://www.kaggle.com/code/whitelily/co-occurrence-baseline\n",
        "[8]: https://www.kaggle.com/datasets/columbia2131/otto-chunk-data-inparquet-format\n",
        "[9]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364721\n",
        "[10]: https://www.kaggle.com/cdeotte/compute-validation-score-cv-564\n",
        "[11]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991"
      ],
      "metadata": {
        "id": "lFndIfVo3MSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Candidate Generation with RAPIDS\n",
        "For candidate generation, we build three co-visitation matrices. One computes the popularity of cart/order given a user's previous click/cart/order. We apply type weighting to this matrix. One computes the popularity of cart/order given a user's previous cart/order. We call this \"buy2buy\" matrix. One computes the popularity of clicks given a user previously click/cart/order.  We apply time weighting to this matrix. We will use RAPIDS cuDF GPU to compute these matrices quickly!"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "6tFv80e23MSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import os, sys, pickle, glob, gc\n",
        "from collections import Counter\n",
        "import cudf, itertools\n",
        "print('We will use RAPIDS version',cudf.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-31T07:14:42.858514Z",
          "iopub.execute_input": "2022-12-31T07:14:42.858939Z",
          "iopub.status.idle": "2022-12-31T07:14:42.879443Z",
          "shell.execute_reply.started": "2022-12-31T07:14:42.858905Z",
          "shell.execute_reply": "2022-12-31T07:14:42.877778Z"
        },
        "trusted": true,
        "id": "ZJmMnS1y3MSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Three Co-visitation Matrices with RAPIDS\n",
        "We will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable `DISK_PIECES` to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation\n",
        "* Use RAPIDS cuDF GPU instead of Pandas CPU\n",
        "* Read disk once and save in CPU RAM for later GPU multiple use\n",
        "* Process largest amount of data possible on GPU at one time\n",
        "* Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n",
        "* Write result as parquet instead of dictionary"
      ],
      "metadata": {
        "id": "HNLWXo0G3MSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# CACHE FUNCTIONS\n",
        "def read_file(f):\n",
        "    return cudf.DataFrame( data_cache[f] )\n",
        "def read_file_to_cache(f):\n",
        "    df = pd.read_parquet(f)\n",
        "    df.ts = (df.ts/1000).astype('int32')\n",
        "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
        "    return df\n",
        "\n",
        "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
        "data_cache = {}\n",
        "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
        "files = glob.glob('../input/otto-chunk-data-inparquet-format/*_parquet/*')\n",
        "for f in files: data_cache[f] = read_file_to_cache(f)\n",
        "\n",
        "# CHUNK PARAMETERS\n",
        "READ_CT = 5\n",
        "CHUNK = int( np.ceil( len(files)/6 ))\n",
        "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
      ],
      "metadata": {
        "id": "mlOlALng3MSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted"
      ],
      "metadata": {
        "id": "aAD8jBfT3MSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
        "DISK_PIECES = 4\n",
        "SIZE = 1.86e6/DISK_PIECES\n",
        "\n",
        "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
        "for PART in range(DISK_PIECES):\n",
        "    print()\n",
        "    print('### DISK PART',PART+1)\n",
        "    \n",
        "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
        "    # => OUTER CHUNKS\n",
        "    for j in range(6):\n",
        "        a = j*CHUNK\n",
        "        b = min( (j+1)*CHUNK, len(files) )\n",
        "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
        "        \n",
        "        # => INNER CHUNKS\n",
        "        for k in range(a,b,READ_CT):\n",
        "            # READ FILE\n",
        "            df = [read_file(files[k])]\n",
        "            for i in range(1,READ_CT): \n",
        "                if k+i<b: df.append( read_file(files[k+i]) )\n",
        "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
        "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
        "            \n",
        "            # USE TAIL OF SESSION\n",
        "            df = df.reset_index(drop=True)\n",
        "            df['n'] = df.groupby('session').cumcount()\n",
        "            df = df.loc[df.n<30].drop('n',axis=1)\n",
        "            \n",
        "            # CREATE PAIRS\n",
        "            df = df.merge(df,on='session')\n",
        "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
        "            \n",
        "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
        "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
        "            \n",
        "            # ASSIGN WEIGHTS\n",
        "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n",
        "            df['wgt'] = df.type_y.map(type_weight)\n",
        "            df = df[['aid_x','aid_y','wgt']]\n",
        "            df.wgt = df.wgt.astype('float32')\n",
        "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
        "            \n",
        "            # COMBINE INNER CHUNKS\n",
        "            if k==a: tmp2 = df\n",
        "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
        "            print(k,', ',end='')\n",
        "        \n",
        "        print()\n",
        "        \n",
        "        # COMBINE OUTER CHUNKS\n",
        "        if a==0: tmp = tmp2\n",
        "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
        "        del tmp2, df\n",
        "        gc.collect()\n",
        "\n",
        "    # CONVERT MATRIX TO DICTIONARY\n",
        "    tmp = tmp.reset_index()\n",
        "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
        "    \n",
        "    # SAVE TOP 40\n",
        "    tmp = tmp.reset_index(drop=True)\n",
        "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
        "    tmp = tmp.loc[tmp.n<carts_th].drop('n',axis=1)\n",
        "    \n",
        "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
        "    tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')"
      ],
      "metadata": {
        "id": "uJjHwq9G3MSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) \"Buy2Buy\" Co-visitation Matrix"
      ],
      "metadata": {
        "id": "SiYB8IQd3MSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
        "DISK_PIECES = 1\n",
        "SIZE = 1.86e6/DISK_PIECES\n",
        "\n",
        "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
        "for PART in range(DISK_PIECES):\n",
        "    print()\n",
        "    print('### DISK PART',PART+1)\n",
        "    \n",
        "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
        "    # => OUTER CHUNKS\n",
        "    for j in range(6):\n",
        "        a = j*CHUNK\n",
        "        b = min( (j+1)*CHUNK, len(files) )\n",
        "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
        "        \n",
        "        # => INNER CHUNKS\n",
        "        for k in range(a,b,READ_CT):\n",
        "            \n",
        "            # READ FILE\n",
        "            df = [read_file(files[k])]\n",
        "            for i in range(1,READ_CT): \n",
        "                if k+i<b: df.append( read_file(files[k+i]) )\n",
        "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
        "            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
        "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
        "            \n",
        "            # USE TAIL OF SESSION\n",
        "            df = df.reset_index(drop=True)\n",
        "            df['n'] = df.groupby('session').cumcount()\n",
        "            df = df.loc[df.n<30].drop('n',axis=1)\n",
        "            \n",
        "            # CREATE PAIRS\n",
        "            df = df.merge(df,on='session')\n",
        "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
        "            \n",
        "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
        "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
        "            \n",
        "            # ASSIGN WEIGHTS\n",
        "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n",
        "            df['wgt'] = 1\n",
        "            df = df[['aid_x','aid_y','wgt']]\n",
        "            df.wgt = df.wgt.astype('float32')\n",
        "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
        "            \n",
        "            # COMBINE INNER CHUNKS\n",
        "            if k==a: tmp2 = df\n",
        "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
        "            print(k,', ',end='')\n",
        "\n",
        "        print()\n",
        "        \n",
        "        # COMBINE OUTER CHUNKS\n",
        "        if a==0: tmp = tmp2\n",
        "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
        "        del tmp2, df\n",
        "        gc.collect()\n",
        "\n",
        "    # CONVERT MATRIX TO DICTIONARY\n",
        "    tmp = tmp.reset_index()\n",
        "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
        "    \n",
        "    # SAVE TOP 40\n",
        "    tmp = tmp.reset_index(drop=True)\n",
        "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
        "    tmp = tmp.loc[tmp.n<orders_th].drop('n',axis=1)\n",
        "    \n",
        "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
        "    tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')"
      ],
      "metadata": {
        "id": "Z89Ai4AW3MSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) \"Clicks\" Co-visitation Matrix - Time Weighted"
      ],
      "metadata": {
        "id": "leSGeg1G3MSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
        "DISK_PIECES = 4\n",
        "SIZE = 1.86e6/DISK_PIECES\n",
        "\n",
        "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
        "for PART in range(DISK_PIECES):\n",
        "    print()\n",
        "    print('### DISK PART',PART+1)\n",
        "    \n",
        "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
        "    # => OUTER CHUNKS\n",
        "    for j in range(6):\n",
        "        a = j*CHUNK\n",
        "        b = min( (j+1)*CHUNK, len(files) )\n",
        "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
        "        \n",
        "        # => INNER CHUNKS\n",
        "        for k in range(a,b,READ_CT):\n",
        "            # READ FILE\n",
        "            df = [read_file(files[k])]\n",
        "            for i in range(1,READ_CT): \n",
        "                if k+i<b: df.append( read_file(files[k+i]) )\n",
        "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
        "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
        "            \n",
        "            # USE TAIL OF SESSION\n",
        "            df = df.reset_index(drop=True)\n",
        "            df['n'] = df.groupby('session').cumcount()\n",
        "            df = df.loc[df.n<30].drop('n',axis=1)\n",
        "            \n",
        "            # CREATE PAIRS\n",
        "            df = df.merge(df,on='session')\n",
        "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
        "            \n",
        "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
        "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
        "            \n",
        "            # ASSIGN WEIGHTS\n",
        "            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
        "            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
        "            # 1659304800 : minimum timestamp\n",
        "            # 1662328791 : maximum timestamp\n",
        "            df = df[['aid_x','aid_y','wgt']]\n",
        "            df.wgt = df.wgt.astype('float32')\n",
        "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
        "            \n",
        "            # COMBINE INNER CHUNKS\n",
        "            if k==a: tmp2 = df\n",
        "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
        "            print(k,', ',end='')\n",
        "        print()\n",
        "        \n",
        "        # COMBINE OUTER CHUNKS\n",
        "        if a==0: tmp = tmp2\n",
        "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
        "        del tmp2, df\n",
        "        gc.collect()\n",
        "\n",
        "    # CONVERT MATRIX TO DICTIONARY\n",
        "    tmp = tmp.reset_index()\n",
        "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
        "    \n",
        "    # SAVE TOP 40\n",
        "    tmp = tmp.reset_index(drop=True)\n",
        "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
        "    tmp = tmp.loc[tmp.n<clicks_th].drop('n',axis=1)\n",
        "    \n",
        "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
        "    tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')"
      ],
      "metadata": {
        "id": "k44rF6-K3MSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FREE MEMORY\n",
        "del data_cache, tmp\n",
        "_ = gc.collect()"
      ],
      "metadata": {
        "id": "uD0mH-IX3MSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - ReRank (choose 20) using handcrafted rules\n",
        "For description of the handcrafted rules, read this notebook's intro."
      ],
      "metadata": {
        "id": "JScnskhe3MSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_test():    \n",
        "    dfs = []\n",
        "    for e, chunk_file in enumerate(glob.glob('../input/otto-chunk-data-inparquet-format/test_parquet/*')):\n",
        "        chunk = pd.read_parquet(chunk_file)\n",
        "        chunk.ts = (chunk.ts/1000).astype('int32')\n",
        "        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n",
        "        dfs.append(chunk)\n",
        "    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n",
        "\n",
        "test_df = load_test()\n",
        "print('Test data has shape',test_df.shape)\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "X_fhktQB3MSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def pqt_to_dict(df):\n",
        "    return df.groupby('aid_x').aid_y.apply(list).to_dict()\n",
        "\n",
        "# LOAD THREE CO-VISITATION MATRICES\n",
        "top_20_clicks = pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_0.pqt') )\n",
        "\n",
        "for k in range(1,DISK_PIECES): \n",
        "    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_{k}.pqt') ) )\n",
        "\n",
        "\n",
        "top_20_buys = pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_0.pqt') )\n",
        "\n",
        "for k in range(1,DISK_PIECES): \n",
        "    top_20_buys.update( pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_{k}.pqt') ) )\n",
        "\n",
        "top_20_buy2buy = pqt_to_dict( pd.read_parquet(f'top_15_buy2buy_v{VER}_0.pqt') )\n",
        "\n",
        "# TOP CLICKS AND ORDERS IN TEST\n",
        "#top_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:20]\n",
        "#top_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:20]\n",
        "\n",
        "print('Here are size of our 3 co-visitation matrices:')\n",
        "print( len( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys ) )"
      ],
      "metadata": {
        "id": "CrY4oWy83MSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_clicks = test_df.loc[test_df['type']== 0,'aid'].value_counts().index.values[:20] \n",
        "top_carts = test_df.loc[test_df['type']== 1,'aid'].value_counts().index.values[:20]\n",
        "top_orders = test_df.loc[test_df['type']== 2,'aid'].value_counts().index.values[:20]"
      ],
      "metadata": {
        "id": "PcyiSnj13MST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_clicks(df):\n",
        "    # USER HISTORY AIDS AND TYPES\n",
        "    aids=df.aid.tolist()\n",
        "    types = df.type.tolist()\n",
        "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
        "    # RERANK CANDIDATES USING WEIGHTS\n",
        "    if len(unique_aids)>=20:\n",
        "        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n",
        "        aids_temp = Counter() \n",
        "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
        "        for aid,w,t in zip(aids,weights,types): \n",
        "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
        "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
        "        return sorted_aids\n",
        "    # USE \"CLICKS\" CO-VISITATION MATRIX\n",
        "    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
        "    # RERANK CANDIDATES\n",
        "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]    \n",
        "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
        "    # USE TOP20 TEST CLICKS\n",
        "    return result + list(top_clicks)[:20-len(result)]"
      ],
      "metadata": {
        "id": "Icg_s0m33MST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_carts(df):\n",
        "    # User history aids and types\n",
        "    aids = df.aid.tolist()\n",
        "    types = df.type.tolist()\n",
        "    \n",
        "    # UNIQUE AIDS AND UNIQUE BUYS\n",
        "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
        "    df = df.loc[(df['type'] == 0)|(df['type'] == 1)]\n",
        "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
        "    \n",
        "    # Rerank candidates using weights\n",
        "    if len(unique_aids) >= 20:\n",
        "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
        "        aids_temp = Counter() \n",
        "        \n",
        "        # Rerank based on repeat items and types of items\n",
        "        for aid,w,t in zip(aids,weights,types): \n",
        "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
        "        \n",
        "        # Rerank candidates using\"top_20_carts\" co-visitation matrix\n",
        "        aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_buys if aid in top_20_buys]))\n",
        "        for aid in aids2: aids_temp[aid] += 0.1\n",
        "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
        "        return sorted_aids\n",
        "    \n",
        "    # Use \"cart order\" and \"clicks\" co-visitation matrices\n",
        "    aids1 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
        "    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n",
        "    \n",
        "    # RERANK CANDIDATES\n",
        "    top_aids2 = [aid2 for aid2, cnt in Counter(aids1+aids2).most_common(20) if aid2 not in unique_aids] \n",
        "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
        "    \n",
        "    # USE TOP20 TEST ORDERS\n",
        "    return result + list(top_carts)[:20-len(result)]"
      ],
      "metadata": {
        "id": "fNIIJ72i3MSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_buys(df):\n",
        "    # USER HISTORY AIDS AND TYPES\n",
        "    aids=df.aid.tolist()\n",
        "    types = df.type.tolist()\n",
        "    # UNIQUE AIDS AND UNIQUE BUYS\n",
        "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
        "    df = df.loc[(df['type']==1)|(df['type']==2)]\n",
        "    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
        "    # RERANK CANDIDATES USING WEIGHTS\n",
        "    if len(unique_aids)>=20:\n",
        "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
        "        aids_temp = Counter() \n",
        "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
        "        for aid,w,t in zip(aids,weights,types): \n",
        "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
        "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
        "        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
        "        for aid in aids3: aids_temp[aid] += 0.1\n",
        "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
        "        return sorted_aids\n",
        "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
        "    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n",
        "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
        "    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
        "    # RERANK CANDIDATES\n",
        "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(20) if aid2 not in unique_aids] \n",
        "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
        "    # USE TOP20 TEST ORDERS\n",
        "    return result + list(top_orders)[:20-len(result)]"
      ],
      "metadata": {
        "id": "Hd_OBOlf3MSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Submission CSV\n",
        "Inferring test data with Pandas groupby is slow. We need to accelerate the following code."
      ],
      "metadata": {
        "id": "hn7oO2gw3MSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "pred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
        "    lambda x: suggest_clicks(x)\n",
        ")\n",
        "\n",
        "pred_df_carts = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
        "    lambda x: suggest_carts(x)\n",
        ")\n",
        "\n",
        "pred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
        "    lambda x: suggest_buys(x)\n",
        ")"
      ],
      "metadata": {
        "id": "EnBUzxvh3MSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
        "orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
        "carts_pred_df = pd.DataFrame(pred_df_carts.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()"
      ],
      "metadata": {
        "id": "duOdjFaB3MSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
        "pred_df.columns = [\"session_type\", \"labels\"]\n",
        "pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n",
        "pred_df.to_csv(\"submission.csv\", index=False)\n",
        "pred_df.head()"
      ],
      "metadata": {
        "id": "yskfnnNp3MSX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}